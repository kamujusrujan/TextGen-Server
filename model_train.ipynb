{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tokenizer_form",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EopmdiPAMvbt"
      },
      "source": [
        "root = \"/content/drive/My Drive/Hobby/Fake News Generator/Articles/\"\n",
        "word_vec_path = '/content/drive/My Drive/Hobby/Fake News Generator/Articles/word_vec.kv'\n",
        "model_path = ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi7HsE8SM3hd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "cd68a733-5fac-4c6c-aa2c-a714536b7873"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from zipfile import ZipFile\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayJVpOcTpOxu"
      },
      "source": [
        "df = pd.read_pickle(root + 'dataset.pkl')\n",
        "df.Y = df.Y.apply(lambda x : 'sos ' + x + ' eos')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZZL21C9FeK_"
      },
      "source": [
        "import pickle \n",
        "with open(root + 'tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43_LzTSfp9zK"
      },
      "source": [
        "corpus_words = [nltk.word_tokenize(i) for i in  list(df.Y) + list(df.X) ] \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U14fzMlaK1oj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "fa85b193-4a2d-4e29-eaa8-e16b0449db72"
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1)\n",
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Y</th>\n",
              "      <th>X</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33309</th>\n",
              "      <td>sos bachelor finale  nick viall proposes to    eos</td>\n",
              "      <td>bachelor finale nick viall propose</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30660</th>\n",
              "      <td>sos reddit thread on france firebomb attack closed over antiislam hate speech  breitbart eos</td>\n",
              "      <td>reddit thread france firebomb attack close antiislam hate speech breitbart</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43205</th>\n",
              "      <td>sos report intel chiefs tell trump that russian operatives claim to have compromising information on him eos</td>\n",
              "      <td>report intel chief tell trump russian operatives claim compromise information</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29516</th>\n",
              "      <td>sos huffington post silent on leaked dnc clinton documents  breitbart eos</td>\n",
              "      <td>huffington post silent leaked dnc clinton document breitbart</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4466</th>\n",
              "      <td>sos listening to this new podcast could save your life on election day  the new york times eos</td>\n",
              "      <td>listen new podcast could save life election day new york time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21049</th>\n",
              "      <td>sos  percent of migrants in germany fit only for menial jobs eos</td>\n",
              "      <td>percent migrant germany fit menial job</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33132</th>\n",
              "      <td>sos america first puts freedom and leadership last eos</td>\n",
              "      <td>america first put freedom leadership last</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33289</th>\n",
              "      <td>sos islamist rebels claim responsibility for twin blasts in damascus eos</td>\n",
              "      <td>islamist rebel claim responsibility twin blast damascus</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47945</th>\n",
              "      <td>sos megyn kelly has a oneword response to eric trumps comment on sexual harassment in the workplace eos</td>\n",
              "      <td>megyn kelly oneword response eric trumps comment sexual harassment workplace</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23931</th>\n",
              "      <td>sos tim tebow celebrates valentines day with proms for special needs people  breitbart eos</td>\n",
              "      <td>tim tebow celebrate valentine day prom special need people breitbart</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                  Y                                                                              X\n",
              "33309  sos bachelor finale  nick viall proposes to    eos                                                            bachelor finale nick viall propose                                           \n",
              "30660  sos reddit thread on france firebomb attack closed over antiislam hate speech  breitbart eos                  reddit thread france firebomb attack close antiislam hate speech breitbart   \n",
              "43205  sos report intel chiefs tell trump that russian operatives claim to have compromising information on him eos  report intel chief tell trump russian operatives claim compromise information\n",
              "29516  sos huffington post silent on leaked dnc clinton documents  breitbart eos                                     huffington post silent leaked dnc clinton document breitbart                 \n",
              "4466   sos listening to this new podcast could save your life on election day  the new york times eos                listen new podcast could save life election day new york time                \n",
              "21049  sos  percent of migrants in germany fit only for menial jobs eos                                              percent migrant germany fit menial job                                       \n",
              "33132  sos america first puts freedom and leadership last eos                                                        america first put freedom leadership last                                    \n",
              "33289  sos islamist rebels claim responsibility for twin blasts in damascus eos                                      islamist rebel claim responsibility twin blast damascus                      \n",
              "47945  sos megyn kelly has a oneword response to eric trumps comment on sexual harassment in the workplace eos       megyn kelly oneword response eric trumps comment sexual harassment workplace \n",
              "23931  sos tim tebow celebrates valentines day with proms for special needs people  breitbart eos                    tim tebow celebrate valentine day prom special need people breitbart         "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mX9gksKNwQ5"
      },
      "source": [
        "### Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQbr5gNlNVEA"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer( oov_token=\"UNK\")\n",
        "tokenizer.fit_on_texts(corpus_words)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eEZfOJEN764",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8d9d1b7-b443-4664-c369-4f7241dc328e"
      },
      "source": [
        "train_X = tokenizer.texts_to_sequences(df['X'])\n",
        "train_X = pad_sequences(train_X,maxlen = 10 , padding = 'post')\n",
        "train_Y = tokenizer.texts_to_sequences(df['Y'])\n",
        "train_Y = pad_sequences(train_Y, maxlen = 30, padding = 'post')\n",
        "train_Y.shape,train_X.shape "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 30), (50000, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRyRdkROPfbK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd71cf4-1788-4fd9-f245-70327cb8db16"
      },
      "source": [
        "train_X = train_X.reshape(50000, 10 ,-1 )\n",
        "train_Y = train_Y.reshape(50000, -1 ,1 )\n",
        "train_X.shape, train_Y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 10, 1), (50000, 30, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3xQnci6Nbes"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zcjDuc1WoJS"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, LSTM , Dropout , RepeatVector , TimeDistributed , Input, Bidirectional , Embedding\n",
        "from tensorflow.keras.models import Sequential, load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3_JeIPFNv0Z"
      },
      "source": [
        "### try bidirectional and attention models\n",
        "### can also should generate given any sequence\n",
        "model = Sequential()\n",
        "model.add(Input(shape = (None,)))\n",
        "model.add(Embedding(input_dim= len(tokenizer.word_counts) + 2, output_dim=100 , input_length=10 ))\n",
        "model.add(LSTM(200))\n",
        "model.add(RepeatVector(30))\n",
        "model.add(LSTM(200 ,return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(len(tokenizer.word_counts) + 2 ) ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHHwNzWHW0FS"
      },
      "source": [
        "model.compile(optimizer='adam' ,\n",
        "              # loss = 'mean_squared_error',) \n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi3fJeerqrfL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29789462-2868-41c2-8263-16b4a3e71d67"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 100)         3156100   \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 200)               240800    \n",
            "_________________________________________________________________\n",
            "repeat_vector_2 (RepeatVecto (None, 30, 200)           0         \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 30, 200)           320800    \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 30, 31561)         6343761   \n",
            "=================================================================\n",
            "Total params: 10,061,461\n",
            "Trainable params: 10,061,461\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4cFZz-jqxI3"
      },
      "source": [
        "model = load_model(root + 'hyper.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKlvW-RMVfDz"
      },
      "source": [
        "def test(text):\n",
        "  test_text = text\n",
        "  temp = tokenizer.texts_to_sequences([test_text])\n",
        "  temp = pad_sequences(temp, maxlen = 10 , padding = 'post')\n",
        "  out = model.predict(temp)\n",
        "  sentence = ['sos']\n",
        "  for word in out[0]:\n",
        "    w = tokenizer.index_word[np.argmax(word)]\n",
        "    if w == 'eos' : break\n",
        "    if w != sentence[-1] : sentence.append(w)\n",
        "  print(' '.join(sentence[1:]))\n",
        "  # sentence = [ np.argmax(word)  for word in out[0]]\n",
        "  # for i in range(1,len(sentence)) : \n",
        "  # decoded = [ tokenizer.index_word[index]  for index in sentence if index!= 0 ]\n",
        "  # print(decoded)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lutHvGI2Y4Za",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6126b6bf-b2ad-40f5-a653-6bdd2fac2139"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sos', 'nfl', 'preview', 'championship', 'matchups', 'prove', 'hillarys', 'without', 'elite', 'qbs', 'sports', 'sports', 'breitbart', 'eos', 'eos']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None,\n",
              " 'sos nfl preview championship matchups prove teams without elite qbs wasting time  breitbart eos')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjfOxdh9rcYt"
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "class customCallback(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    test(df.X[9876])\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rVtpJkLXMny"
      },
      "source": [
        "model.fit(train_X,train_Y, batch_size=10, epochs=20 , callbacks=[customCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oVPpfuZJ3TH"
      },
      "source": [
        "model.save(root + 'hyper_v1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCZs0RhTL7Wp"
      },
      "source": [
        "tokenizer.oov_token = 'UNK'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AqbR0baVrPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8430d955-e52d-44d2-9142-2c88d6ec1b2c"
      },
      "source": [
        "test(\"Trump gone wild\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trumps his is guy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxLda7aNhE_D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d391c82-9cf4-470d-8b26-126fbb861c58"
      },
      "source": [
        "tokenizer.sequences_to_texts(np.array(train_Y[0]).reshape(1,-1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['house republicans fret about winning their health care suit the new york times']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abxDtinINXDk"
      },
      "source": [
        "### Preprocessing and loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJbKjYpCNDhi"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import common_texts, get_tmpfile\n",
        "# word_2_vec.wv.save(word_vec_path)\n",
        "word_2_vec = KeyedVectors.load(word_vec_path, mmap='r')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYqdWfsNM-8m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7FE7jlQNK6x"
      },
      "source": [
        "df['vec_x'] = df.X.apply(lambda x : np.array([ word_2_vec[i] for i in nltk.word_tokenize(x)]))\n",
        "df['vec_y'] = df.X.apply(lambda x : np.array([ word_2_vec[i] for i in nltk.word_tokenize(x)]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH9BE-wANRF5"
      },
      "source": [
        "X_train  = pad_sequences(df.vec_x, maxlen=10 , dtype= 'float32' , padding = 'post')\n",
        "Y_train = pad_sequences(df.vec_y, maxlen=20, dtype= 'float32' ,padding= 'post')\n",
        "X_train.shape,Y_train.shape "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPvJ4Txpc7pP"
      },
      "source": [
        "### New model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XACbZkVXc9Ld"
      },
      "source": [
        "# attention model? \n",
        "# random attention model to generate different texts\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}